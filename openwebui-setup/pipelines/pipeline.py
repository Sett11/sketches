

import os
import json
import logging
from typing import Dict, List, Any, Optional
from pathlib import Path
import psycopg2
from psycopg2.extras import RealDictCursor
import numpy as np
from sentence_transformers import SentenceTransformer
import requests
from pydantic import BaseModel

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LegalPipeline:
    """Pipeline –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏"""
    
    def __init__(self):
        self.model = None
        self.db_connection = None
        self.setup_database()
        self.load_embedding_model()
    
    def setup_database(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –≤–Ω–µ—à–Ω–µ–π –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö PostgreSQL —Å pgvector"""
        try:
            # –ü–∞—Ä—Å–∏–º DB_URL –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ö–æ—Å—Ç–∞ –∏ –ø–æ—Ä—Ç–∞
            db_url = os.getenv('DB_URL', '')
            if ':' in db_url:
                host, port = db_url.split(':')
            else:
                host = db_url
                port = '5432'
            
            self.db_connection = psycopg2.connect(
                host=host,
                port=port,
                database=os.getenv('DB_NAME'),
                user=os.getenv('DB_USER'),
                password=os.getenv('DB_PASSWORD')
            )
            logger.info(f"‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –≤–Ω–µ—à–Ω–µ–π –ë–î —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ: {host}:{port}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î: {e}")
            self.db_connection = None
    
    def load_embedding_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        try:
            self.model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
            logger.info("‚úÖ –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            self.model = None
    
    def create_embeddings(self, text: str) -> Optional[List[float]]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        if not self.model:
            return None
        try:
            embedding = self.model.encode(text)
            return embedding.tolist()
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
            return None
    
    def search_similar_documents(self, query: str, embedding_type: str = "all", limit: int = 5) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ —Ç–∏–ø–∞–º —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        if not self.db_connection or not self.model:
            return []
        
        try:
            query_embedding = self.create_embeddings(query)
            if not query_embedding:
                return []
            
            cursor = self.db_connection.cursor(cursor_factory=RealDictCursor)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∞–±–ª–∏—Ü—É –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
            if embedding_type == "small":
                table_name = "paragraphs"  # –ú–∞–ª—ã–µ - –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
            elif embedding_type == "medium":
                table_name = "articles"    # –°—Ä–µ–¥–Ω–∏–µ - —Å—Ç–∞—Ç—å–∏
            elif embedding_type == "large":
                table_name = "chapters"    # –ë–æ–ª—å—à–∏–µ - –≥–ª–∞–≤—ã
            else:
                # –ü–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º —Ç–∏–ø–∞–º
                results = []
                for table in ["paragraphs", "articles", "chapters"]:
                    table_results = self._search_in_table(cursor, table, query_embedding, limit // 3)
                    results.extend(table_results)
                cursor.close()
                return sorted(results, key=lambda x: x['similarity'], reverse=True)[:limit]
            
            # –ü–æ–∏—Å–∫ –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ç–∞–±–ª–∏—Ü–µ
            results = self._search_in_table(cursor, table_name, query_embedding, limit)
            cursor.close()
            return results
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –ë–î: {e}")
            return []
    
    def _search_in_table(self, cursor, table_name: str, query_embedding: List[float], limit: int) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ç–∞–±–ª–∏—Ü–µ"""
        try:
            cursor.execute(f"""
                SELECT 
                    id,
                    title,
                    content,
                    document_path,
                    created_at,
                    embedding_type,
                    1 - (embedding <=> %s) as similarity
                FROM {table_name} 
                WHERE embedding IS NOT NULL
                ORDER BY embedding <=> %s
                LIMIT %s
            """, (query_embedding, query_embedding, limit))
            
            results = cursor.fetchall()
            return [dict(row) for row in results]
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ —Ç–∞–±–ª–∏—Ü–µ {table_name}: {e}")
            return []
    
    def analyze_legal_document(self, document_path: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
            if not os.path.exists(document_path):
                return {"error": "–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω"}
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∞–π–ª–µ
            file_info = {
                "path": document_path,
                "name": os.path.basename(document_path),
                "size": os.path.getsize(document_path),
                "extension": os.path.splitext(document_path)[1].lower()
            }
            
            # –ï—Å–ª–∏ —ç—Ç–æ PDF, –ø—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç
            if file_info["extension"] == ".pdf":
                # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF
                # –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                file_info["type"] = "PDF –¥–æ–∫—É–º–µ–Ω—Ç"
                file_info["status"] = "–ì–æ—Ç–æ–≤ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ"
            else:
                file_info["type"] = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç"
                file_info["status"] = "–¢—Ä–µ–±—É–µ—Ç –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏"
            
            return file_info
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
            return {"error": str(e)}
    
    def answer_legal_question(self, question: str, context_documents: List[Dict] = None) -> Dict[str, Any]:
        """–û—Ç–≤–µ—Ç –Ω–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å"""
        try:
            # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            relevant_docs = self.search_similar_documents(question, limit=3)
            
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            context = ""
            if relevant_docs:
                context = "\n\n".join([
                    f"–î–æ–∫—É–º–µ–Ω—Ç: {doc['title']}\n–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {doc['content'][:500]}..."
                    for doc in relevant_docs
                ])
            
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
            response = {
                "question": question,
                "relevant_documents": relevant_docs,
                "context": context,
                "answer": f"–ù–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {context[:200]}..." if context else "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã",
                "confidence": len(relevant_docs) / 3.0 if relevant_docs else 0.0
            }
            
            return response
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–æ–ø—Ä–æ—Å–∞: {e}")
            return {"error": str(e)}
    
    def extract_legal_entities(self, text: str) -> Dict[str, List[str]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∞–≤–æ–≤—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        try:
            # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (–º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å —Å –ø–æ–º–æ—â—å—é NER)
            legal_keywords = [
                "–¥–æ–≥–æ–≤–æ—Ä", "—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ", "–∑–∞–∫–æ–Ω", "—Å—Ç–∞—Ç—å—è", "–ø—É–Ω–∫—Ç", "–ø–∞—Ä–∞–≥—Ä–∞—Ñ",
                "—Å—Ç–æ—Ä–æ–Ω–∞", "–æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ", "–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å", "–ø—Ä–∞–≤–æ", "–æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç—å",
                "—Å—Ä–æ–∫", "—É—Å–ª–æ–≤–∏–µ", "—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ", "–∑–∞—è–≤–ª–µ–Ω–∏–µ", "–∏—Å–∫", "—Å—É–¥"
            ]
            
            found_entities = []
            for keyword in legal_keywords:
                if keyword.lower() in text.lower():
                    found_entities.append(keyword)
            
            return {
                "legal_entities": found_entities,
                "count": len(found_entities),
                "text_length": len(text)
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π: {e}")
            return {"error": str(e)}

# –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ Pipeline
pipeline = LegalPipeline()

def pipe(request: Dict[str, Any]) -> Dict[str, Any]:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤"""
    try:
        action = request.get("action", "analyze")
        data = request.get("data", {})
        
        logger.info(f"üîç –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {action}")
        
        if action == "analyze_document":
            document_path = data.get("document_path")
            if not document_path:
                return {"error": "–ù–µ —É–∫–∞–∑–∞–Ω –ø—É—Ç—å –∫ –¥–æ–∫—É–º–µ–Ω—Ç—É"}
            
            result = pipeline.analyze_legal_document(document_path)
            return {"action": action, "result": result}
        
        elif action == "search_documents":
            query = data.get("query", "")
            embedding_type = data.get("embedding_type", "all")  # small, medium, large, all
            limit = data.get("limit", 5)
            
            results = pipeline.search_similar_documents(query, embedding_type, limit)
            return {"action": action, "results": results, "count": len(results), "embedding_type": embedding_type}
        
        elif action == "answer_question":
            question = data.get("question", "")
            if not question:
                return {"error": "–ù–µ —É–∫–∞–∑–∞–Ω –≤–æ–ø—Ä–æ—Å"}
            
            result = pipeline.answer_legal_question(question)
            return {"action": action, "result": result}
        
        elif action == "extract_entities":
            text = data.get("text", "")
            if not text:
                return {"error": "–ù–µ —É–∫–∞–∑–∞–Ω —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"}
            
            result = pipeline.extract_legal_entities(text)
            return {"action": action, "result": result}
        
        else:
            return {"error": f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ: {action}"}
    
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}")
        return {"error": str(e)}

def on_startup():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ"""
    logger.info("üöÄ –Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–π Pipeline –∑–∞–ø—É—â–µ–Ω")
    logger.info(f"üìÅ –ü–∞–ø–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {os.getenv('LEGAL_DOCUMENTS_PATH', '/app/legal_documents')}")
    logger.info(f"üóÑÔ∏è –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö: {os.getenv('DB_NAME', 'legal_db')}")

def on_shutdown():
    """–û—á–∏—Å—Ç–∫–∞ –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ"""
    if pipeline.db_connection:
        pipeline.db_connection.close()
    logger.info("üõë –Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–π Pipeline –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
