import os
import json
import logging
import asyncio
from typing import Dict, List, Any, Optional
from pathlib import Path
import psycopg2
from psycopg2.extras import RealDictCursor
import numpy as np
from sentence_transformers import SentenceTransformer
import requests
from mcp.server import Server
from mcp.server.stdio import stdio_server
from mcp.types import Tool, TextContent
from mcp.server import NotificationOptions

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LegalMCPServer:
    """MCP —Å–µ—Ä–≤–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏"""
    
    def __init__(self):
        self.model = None
        self.db_connection = None
        self.setup_database()
        self.load_embedding_model()
    
    def setup_database(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –≤–Ω–µ—à–Ω–µ–π –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö PostgreSQL —Å pgvector"""
        try:
            # –ü–∞—Ä—Å–∏–º DB_URL –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ö–æ—Å—Ç–∞ –∏ –ø–æ—Ä—Ç–∞
            db_url = os.getenv('DB_URL', '')
            if ':' in db_url:
                host, port = db_url.split(':')
            else:
                host = db_url
                port = '5432'
            
            self.db_connection = psycopg2.connect(
                host=host,
                port=port,
                database=os.getenv('DB_NAME'),
                user=os.getenv('DB_USER'),
                password=os.getenv('DB_PASSWORD')
            )
            logger.info(f"‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –≤–Ω–µ—à–Ω–µ–π –ë–î —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ: {host}:{port}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î: {e}")
            self.db_connection = None
    
    def load_embedding_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        try:
            self.model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
            logger.info("‚úÖ –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            self.model = None
    
    def create_embeddings(self, text: str) -> Optional[List[float]]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        if not self.model:
            return None
        try:
            embedding = self.model.encode(text)
            return embedding.tolist()
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
            return None
    
    def search_similar_documents(self, query: str, embedding_type: str = "all", limit: int = 5) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ —Ç–∏–ø–∞–º —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        if not self.db_connection or not self.model:
            return []
        
        try:
            query_embedding = self.create_embeddings(query)
            if not query_embedding:
                return []
            
            cursor = self.db_connection.cursor(cursor_factory=RealDictCursor)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∞–±–ª–∏—Ü—É –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
            if embedding_type == "small":
                table_name = "paragraphs"  # –ú–∞–ª—ã–µ - –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
            elif embedding_type == "medium":
                table_name = "articles"    # –°—Ä–µ–¥–Ω–∏–µ - —Å—Ç–∞—Ç—å–∏
            elif embedding_type == "large":
                table_name = "chapters"    # –ë–æ–ª—å—à–∏–µ - –≥–ª–∞–≤—ã
            else:
                # –ü–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º —Ç–∏–ø–∞–º
                results = []
                for table in ["paragraphs", "articles", "chapters"]:
                    table_results = self._search_in_table(cursor, table, query_embedding, limit // 3)
                    results.extend(table_results)
                cursor.close()
                return sorted(results, key=lambda x: x['similarity'], reverse=True)[:limit]
            
            # –ü–æ–∏—Å–∫ –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ç–∞–±–ª–∏—Ü–µ
            results = self._search_in_table(cursor, table_name, query_embedding, limit)
            cursor.close()
            return results
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –ë–î: {e}")
            return []
    
    def _search_in_table(self, cursor, table_name: str, query_embedding: List[float], limit: int) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ç–∞–±–ª–∏—Ü–µ"""
        try:
            cursor.execute(f"""
                SELECT 
                    id,
                    title,
                    content,
                    document_path,
                    created_at,
                    embedding_type,
                    1 - (embedding <=> %s) as similarity
                FROM {table_name} 
                WHERE embedding IS NOT NULL
                ORDER BY embedding <=> %s
                LIMIT %s
            """, (query_embedding, query_embedding, limit))
            
            results = cursor.fetchall()
            return [dict(row) for row in results]
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ —Ç–∞–±–ª–∏—Ü–µ {table_name}: {e}")
            return []
    
    def analyze_legal_document(self, document_path: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
            if not os.path.exists(document_path):
                return {"error": "–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω"}
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∞–π–ª–µ
            file_info = {
                "path": document_path,
                "name": os.path.basename(document_path),
                "size": os.path.getsize(document_path),
                "extension": os.path.splitext(document_path)[1].lower()
            }
            
            # –ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            if file_info["extension"] == ".pdf":
                file_info["type"] = "PDF –¥–æ–∫—É–º–µ–Ω—Ç"
                file_info["status"] = "–î–æ–∫—É–º–µ–Ω—Ç —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∏ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"
            else:
                file_info["type"] = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç"
                file_info["status"] = "–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è"
            
            return file_info
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
            return {"error": str(e)}
    
    def answer_legal_question(self, question: str, context_documents: List[Dict] = None) -> Dict[str, Any]:
        """–û—Ç–≤–µ—Ç –Ω–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å"""
        try:
            # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            relevant_docs = self.search_similar_documents(question, limit=3)
            
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            context = ""
            if relevant_docs:
                context = "\n\n".join([
                    f"–î–æ–∫—É–º–µ–Ω—Ç: {doc['title']}\n–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {doc['content'][:500]}..."
                    for doc in relevant_docs
                ])
            
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
            response = {
                "question": question,
                "relevant_documents": relevant_docs,
                "context": context,
                "answer": f"–ù–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {context[:200]}..." if context else "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã",
                "confidence": len(relevant_docs) / 3.0 if relevant_docs else 0.0
            }
            
            return response
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–æ–ø—Ä–æ—Å–∞: {e}")
            return {"error": str(e)}
    
    def extract_legal_entities(self, text: str) -> Dict[str, List[str]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∞–≤–æ–≤—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        try:
            # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (–º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å —Å –ø–æ–º–æ—â—å—é NER)
            legal_keywords = [
                "–¥–æ–≥–æ–≤–æ—Ä", "—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ", "–∑–∞–∫–æ–Ω", "—Å—Ç–∞—Ç—å—è", "–ø—É–Ω–∫—Ç", "–ø–∞—Ä–∞–≥—Ä–∞—Ñ",
                "—Å—Ç–æ—Ä–æ–Ω–∞", "–æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ", "–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å", "–ø—Ä–∞–≤–æ", "–æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç—å",
                "—Å—Ä–æ–∫", "—É—Å–ª–æ–≤–∏–µ", "—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ", "–∑–∞—è–≤–ª–µ–Ω–∏–µ", "–∏—Å–∫", "—Å—É–¥"
            ]
            
            found_entities = []
            for keyword in legal_keywords:
                if keyword.lower() in text.lower():
                    found_entities.append(keyword)
            
            return {
                "legal_entities": found_entities,
                "count": len(found_entities),
                "text_length": len(text)
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π: {e}")
            return {"error": str(e)}

# –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ —Å–µ—Ä–≤–µ—Ä–∞
legal_server = LegalMCPServer()

# –°–æ–∑–¥–∞–Ω–∏–µ MCP —Å–µ—Ä–≤–µ—Ä–∞
server = Server("legal-documents-mcp")

@server.list_tools()
async def list_tools() -> List[Tool]:
    """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤"""
    return [
        Tool(
            name="analyze_legal_document",
            description="–ê–Ω–∞–ª–∏–∑ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (PDF, —Ç–µ–∫—Å—Ç)",
            inputSchema={
                "type": "object",
                "properties": {
                    "document_path": {
                        "type": "string",
                        "description": "–ü—É—Ç—å –∫ –¥–æ–∫—É–º–µ–Ω—Ç—É –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"
                    }
                },
                "required": ["document_path"]
            }
        ),
        Tool(
            name="search_legal_documents",
            description="–ü–æ–∏—Å–∫ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ç–∏–ø–æ–≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"
                    },
                    "embedding_type": {
                        "type": "string",
                        "description": "–¢–∏–ø —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: small (–ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã), medium (—Å—Ç–∞—Ç—å–∏), large (–≥–ª–∞–≤—ã), all (–≤—Å–µ)",
                        "enum": ["small", "medium", "large", "all"],
                        "default": "all"
                    },
                    "limit": {
                        "type": "integer",
                        "description": "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤",
                        "default": 5
                    }
                },
                "required": ["query"]
            }
        ),
        Tool(
            name="answer_legal_question",
            description="–û—Ç–≤–µ—Ç –Ω–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–∞–∑—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤",
            inputSchema={
                "type": "object",
                "properties": {
                    "question": {
                        "type": "string",
                        "description": "–Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å"
                    }
                },
                "required": ["question"]
            }
        ),
        Tool(
            name="extract_legal_entities",
            description="–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∞–≤–æ–≤—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ —Ç–µ–∫—Å—Ç–∞",
            inputSchema={
                "type": "object",
                "properties": {
                    "text": {
                        "type": "string",
                        "description": "–¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"
                    }
                },
                "required": ["text"]
            }
        )
    ]

@server.call_tool()
async def call_tool(name: str, arguments: Dict[str, Any]) -> List[TextContent]:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–∑–æ–≤–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤"""
    try:
        if name == "analyze_legal_document":
            document_path = arguments.get("document_path")
            if not document_path:
                return [TextContent(type="text", text="‚ùå –ù–µ —É–∫–∞–∑–∞–Ω –ø—É—Ç—å –∫ –¥–æ–∫—É–º–µ–Ω—Ç—É")]
            
            result = legal_server.analyze_legal_document(document_path)
            return [TextContent(type="text", text=json.dumps(result, ensure_ascii=False, indent=2))]
        
        elif name == "search_legal_documents":
            query = arguments.get("query", "")
            embedding_type = arguments.get("embedding_type", "all")
            limit = arguments.get("limit", 5)
            
            if not query:
                return [TextContent(type="text", text="‚ùå –ù–µ —É–∫–∞–∑–∞–Ω –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å")]
            
            results = legal_server.search_similar_documents(query, embedding_type, limit)
            return [TextContent(type="text", text=json.dumps({
                "query": query,
                "embedding_type": embedding_type,
                "results": results,
                "count": len(results)
            }, ensure_ascii=False, indent=2))]
        
        elif name == "answer_legal_question":
            question = arguments.get("question", "")
            if not question:
                return [TextContent(type="text", text="‚ùå –ù–µ —É–∫–∞–∑–∞–Ω –≤–æ–ø—Ä–æ—Å")]
            
            result = legal_server.answer_legal_question(question)
            return [TextContent(type="text", text=json.dumps(result, ensure_ascii=False, indent=2))]
        
        elif name == "extract_legal_entities":
            text = arguments.get("text", "")
            if not text:
                return [TextContent(type="text", text="‚ùå –ù–µ —É–∫–∞–∑–∞–Ω —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")]
            
            result = legal_server.extract_legal_entities(text)
            return [TextContent(type="text", text=json.dumps(result, ensure_ascii=False, indent=2))]
        
        else:
            return [TextContent(type="text", text=f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç: {name}")]
    
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ {name}: {e}")
        return [TextContent(type="text", text=f"‚ùå –û—à–∏–±–∫–∞: {str(e)}")]

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞"""
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ Legal Documents MCP Server")
    
    async with stdio_server() as (read_stream, write_stream):
        await server.run(
            read_stream,
            write_stream,
            server.get_capabilities(
                notification_options=NotificationOptions(),
                experimental_capabilities={}
            )
        )

if __name__ == "__main__":
    asyncio.run(main())
